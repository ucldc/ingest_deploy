---
# tasks file for provision_worker
- name: include vaulted passwords from vaulted.yml
  include_vars: vaulted.yml
- name: get correct urls for environment
  include_vars:  "main{{ name_suffix }}.yml"
- debug: var=couchdb_url
- debug: var=solr_url
- debug: var=redis_host
- debug: var=rq_work_queues
- name: install ssh private key for bitbucket
  template: src=id_rsa mode=600 dest=~/.ssh/id_rsa 
- name: install ssh pub key for bitbucket
  copy: src=id_rsa.pub mode=600 dest=~/.ssh/id_rsa.pub 
- { include: install_packages.yml }
- name: make user ~/bin directory
  file: path=/home/{{ ansible_user }}/bin state=directory
- name: install dbus dependencies
  yum: 
    pkg: dbus-glib-devel 
    state: present
  become: yes
- name: create run termination check script
  template:
    src: run_handle_termination.sh.j2
    dest: "/home/{{ ansible_user }}/bin/run_handle_termination.sh"
    mode: "0775"
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: create termination check script
  template:
    src: handle-termination.sh.j2
    dest: "/home/{{ ansible_user }}/bin/handle-termination.sh"
    mode: "0775"
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: install monit
  yum:
    pkg: monit
    state: present
  become: yes
- name: install monit.conf
  template:
    src: monit.conf.j2
    dest: /etc/monit.conf
  become: yes
  notify: reload monit
- name: install handle-termination monit script
  template:
    src: handle-termination.monit.j2
    dest: /etc/monit.d/handle-termination.monit
  become: yes
  notify: reload monit
- name: make virtualenv directory
  file: path={{ dir_venv }} state=directory
- name: pull ingest harvester code to local box
  git:
    repo: https://github.com/ucldc/harvester.git
    dest: "{{ dir_code }}/harvester"
    update: yes
- name: pull dpla based code to local box
  git: 
    repo: git@bitbucket.org:mredar/dpla-ingestion.git
    dest: "{{ dir_code }}/dpla/ingestion"
    accept_hostkey: yes
    version: ucldc
    update: yes
- name: install akara and harvester dpla dependencies
  pip:
    state: present
    requirements: "{{ dir_code }}/dpla/ingestion/requirements.txt"
    virtualenv: "{{ dir_venv }}"
- name: put .boto in place for strict cert checking fix
# https://github.com/boto/boto/issues/2836
  copy: src=dot_boto dest=~/.boto mode=600
- name: install harvester code
  command: "{{ dir_venv }}/bin/python {{ dir_code }}/harvester/setup.py install"
  args:
    chdir: "{{ dir_code }}/harvester"
- name: install Nuxeo deep harvesting code
  include: install_deep_harvester.yml
## configure solr-index-to-s3
- name: create run_dir_solr run dir
  become: yes
  file:
    path: "{{ dir_run }}/{{ run_dir_solr }}"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: create run_dir_solr log dir
  become: yes
  file:
    path: "{{ dir_run }}/{{ run_dir_solr }}/log"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: put solr-index-to-s3 in dir_bin
  become: yes
  template:
      src: solr-index-to-s3.sh.j2
      dest: "{{dir_bin}}/solr-index-to-s3.sh"
      mode: "0775"
      owner: "{{ ansible_user }}"
      group: "{{ ansible_user }}"
- name: put solr-update in dir_bin
  become: yes
  template:
      src: solr-update.sh.j2
      dest: "{{dir_bin}}/solr-update.sh"
      mode: "0775"
      owner: "{{ ansible_user }}"
      group: "{{ ansible_user }}"
################################################################################
#configure akara
################################################################################
- name: copy config file to akara dir???? configure & start akara
  template:
    src: akara.ini.j2
    dest: "{{ dir_code }}/dpla/ingestion/akara.ini"
# configure and start akara
- name: create akara.conf
  command: "{{ dir_venv }}/bin/python setup.py install"
  args:
    chdir: "{{ dir_code }}/dpla/ingestion"
- name: make akara run dir
  become: yes
  file:
    path: "{{ dir_run_akara }}"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: setup akara run dir
  command: "{{ dir_venv}}/bin/akara -f {{ dir_code }}/dpla/ingestion/akara.conf setup"
  args:
    chdir:  "{{ dir_run_akara }}"
- name: run akara
  command: "{{ dir_venv}}/bin/akara -f {{ dir_code }}/dpla/ingestion/akara.conf start"
  args:
    chdir:  "{{ dir_run_akara }}"
    creates: "{{ dir_run_akara }}/logs/akara.pid"
################################################################################
#configure rq worker
################################################################################
- name: make rqworker run dir
  become: yes
  file:
    path: "{{ dir_run_rqworker }}"
    state: directory
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: make profiles dir
  file:
    path: "{{ dir_run_rqworker }}/profiles"
    state: directory
- name: make logs dir
  file:
    path: "{{ dir_run_rqworker }}/logs"
    state: directory
- name: create pynuxrc for the ec2-user in home dir
  template:
    src: pynuxrc
    dest: ~/.pynuxrc
- name: create rqw-settings.py
  template:
    src: rqw-settings.py.j2
    dest: "{{ dir_run_rqworker }}/rqw-settings.py"
- name: put harvester-env in place
  template:
    src: harvester-env.j2
    dest: "{{ dir_run_rqworker}}/.harvester-env"
    mode: "0600"
- name: put start rqworker file in place
  become: yes
  template:
    src: start-rqworker.sh.j2
    dest: "{{ dir_bin }}start-rqworker.sh"
    mode: "0775"
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: put stop rqworker file in place
  become: yes
  template:
    src: stop-rqworker.sh.j2
    dest: "{{ dir_bin }}stop-rqworker.sh"
    mode: "0775"
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
- name: halt.local to call stop rqworker in place
  become: yes
  template:
    src: halt.local.j2
    dest: /sbin/halt.local
    mode: "0775"
- name: start rqworker
  become: no
  shell: "{{ dir_bin }}start-rqworker.sh"
  args:
    creates: "{{ dir_run_rqworker}}/rqworker.pid"
- name: monit validate
  command: monit validate
  become: yes
